Architecture and Setup
Overview:
This project is built to assist users with financial queries through a multi-agent system. The agents include:

Retriever Agent: Retrieves relevant information from documents.

API Agent: Fetches real-time market data using external APIs (such as Yahoo Finance).

Scraping Agent: Downloads and processes SEC filings (e.g., 10-K reports).

Language Agent: Uses a language model (GPT-2) to provide natural language responses to queries.

Framework and Tools:
Backend: FastAPI is used for handling API requests and orchestrating different agents.

Frontend: The UI is built with Streamlit to allow users to interact with the system seamlessly.

Language Model: GPT-2 is used for answering natural language queries, which is handled by the Language Agent.

API Integration: External APIs (Yahoo Finance, SEC filings) are integrated to provide real-time market data.

Data Retrieval: FAISS is used for building and querying an index of relevant documents.

Deployment:
Streamlit Cloud: The project is deployed using Streamlit Cloud, which allows for easy deployment of Python-based web applications. The backend FastAPI service is orchestrated through Streamlit, where all agents interact with one another and provide responses based on user inputs.

Performance Benchmarks:
Response Time: The average time for an agent to respond depends on the complexity of the query and the data being retrieved. For instance, SEC filings may take slightly longer due to the size of the documents being processed.

Scalability: The system is designed to handle multiple concurrent queries, but additional optimizations might be required to scale further (e.g., caching, async operations for large data).


Setup Instructions:
Clone the Repository:
git clone https://github.com/eeshakrishna/RagaAI.git
Install Dependencies:
Create and activate a virtual environment:
python3 -m venv venv
source venv/bin/activate  
Install the required Python packages:
pip install -r requirements.txt
Run the Application:
After setting up the environment and dependencies, run the application using Streamlit:
streamlit run streamlit_app/app.py
To run the FastAPI backend:
uvicorn orchestrator:app --host 0.0.0.0 --port 8000



LangGraph Flow
The LangGraph system represents a multi-agent architecture that is designed to handle user queries related to financial data, process them through a series of agents, and return a response using an AI model (such as GPT-2). Here's a step-by-step explanation of the flow of the LangGraph process:

LangGraph Agent Flow:
User Input (Query):

The system starts when the user inputs a query via the frontend (Streamlit in this case).

The query can be related to any financial aspect like market data, SEC filings, or general financial information.

Retriever Agent:

The query is first passed to the Retriever Agent.

The Retriever Agent uses FAISS or similar techniques to search for relevant documents (e.g., historical stock data, financial reports) based on the query.

The relevant documents are retrieved and returned to the next stage.

API and Scraping Agents:

After retrieving the relevant documents, the system fetches additional data using external APIs (e.g., Yahoo Finance for stock prices) and web scraping (e.g., SEC filings).

API Agent: Fetches real-time financial data based on the query (such as historical stock data or other market-related information).

Scraping Agent: Retrieves detailed filings (e.g., 10-K reports) for specific companies if necessary.

Combine Context:

The retrieved documents from the Retriever Agent are combined with the data fetched from API and Scraping Agents.

This combined context is then prepared to be passed to the next stage for processing.

Language Agent (GPT-2):

The Language Agent (powered by GPT-2 or another language model) receives the combined context (documents + real-time data).

The language model is tasked with interpreting the query and generating a detailed, natural language response.

It processes the context to generate a coherent narrative or answer that directly addresses the user query.

Response to User:

The final response generated by the Language Agent is returned to the frontend (Streamlit) for display to the user.

End:

The process completes with the user receiving the response from the system.



+-------------------+     +------------------+     +-------------------+     +---------------------+
|    User Query     |---->| Retriever Agent  |---->|  API Agent        |---->| Scraping Agent      |
+-------------------+     +------------------+     +-------------------+     +---------------------+
        |                      |                          |                        |
        v                      v                          v                        v
+---------------------+  +---------------------+    +---------------------+   +---------------------+
| Combine Context     |  | Combined Data (Text +|-->| Combined Data (API  |-->| Combined Data (File  |
| (Retriever + API +  |  | Financial Data +     |    | Data + Scraped Data)|   | Data + Context)      |
| Scraped Data)       |  +---------------------+    +---------------------+   +---------------------+
        |                                |
        v                                v
+---------------------+     +---------------------+     
|  Language Agent     |<----|  Generate Response  |
| (GPT-2 Model)      |     |  from Context       |
+---------------------+     +---------------------+     
        |
        v
+---------------------+
|  Final Response     |
|    (to User)        |
+---------------------+
